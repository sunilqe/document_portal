{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5742b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done\n"
     ]
    }
   ],
   "source": [
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100f5d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3954cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3545a0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is asking for the capital of France. Let me think... I remember that France is a country in Western Europe. The capital is a major city there. I've heard Paris mentioned a lot in relation to France. Isn't Paris known for the Eiffel Tower and the Louvre? Yeah, that's right. So, the capital must be Paris.\\n\\nWait, but maybe I should double-check to be sure. Sometimes countries have capitals that aren't their largest cities, but in this case, Paris is both the capital and the largest city in France. I don't think there's any confusion with other cities like Lyon or Marseille. No, Paris is definitely the capital. I think that's correct. Let me see if there's any other possibility. No, I can't recall any other city being the capital. So the answer should be Paris.\\n</think>\\n\\nThe capital of France is **Paris**. It is not only the largest city in France but also a major cultural, historical, and economic center, renowned for landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 233, 'prompt_tokens': 15, 'total_tokens': 248, 'completion_time': 0.508038427, 'prompt_time': 0.000406064, 'queue_time': 0.06614323600000001, 'total_time': 0.508444491}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--80c0e441-7ab0-4a2f-80e9-729102ed208c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 233, 'total_tokens': 248})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5513fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046f79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1156b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_model.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0655394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f985afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88dc96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ed86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5538907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ac0f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd344de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f73ef64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdfb2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52fe3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957b1f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0de5e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dcd3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaaab14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vector_store.similarity_search(\"llam2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae617b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc = vector_store.similarity_search(\"llam2 finetuning benchmark experiments\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd205f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6009a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e63c925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f1575420-e0e2-45d8-9541-22e7f7549ae9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\sunil\\\\Documents\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='3a2f7038-3ad5-4810-9e3d-da291b76d5f3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\sunil\\\\Documents\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='ce38dca6-5209-45ff-985f-afde4aaa68b1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\sunil\\\\Documents\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='f905a7de-bae2-4ea6-aa8d-1e2ce772518d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\sunil\\\\Documents\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf0aa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0752a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5637737",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0105f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abb49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "316771d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb9f6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5334a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, let\\'s see. The user is asking about the Llama 2 fine-tuning benchmark experiments. I need to look at the context provided to find relevant information.\\n\\nLooking at the tables, there are two main sections. The first table (Table 3) compares overall performance on academic benchmarks, and the second part shows \"Fine-tuned\" results. The context mentions that for the models evaluated, the best score between their framework and public results is chosen. The benchmarks are grouped into categories, and individual results are in Section A.2.2, but that\\'s not available here.\\n\\nIn the \"Fine-tuned\" section under Llama 2, there are numbers for different model sizes (7B, 13B, 34B, 70B). These numbers might represent performance metrics across various benchmarks. For example, the 70B model has values like 0.31, 0.29, etc. The user wants to know about the fine-tuning experiments, so I should focus on these numbers and the context around them.\\n\\nThe context also mentions that safety benchmarks are in another section, so maybe these numbers are for different categories like math, coding, or language understanding. Since the user asked about the experiments, I should explain that the results show performance after fine-tuning, but without the actual benchmark names, it\\'s hard to specify. Also, the note says that the best scores are selected, which might mean they optimized for the highest performance.\\n\\nI need to check if the context has enough info. The tables have numbers but no labels for the columns. The user might be expecting details on how they were fine-tuned, but the context doesn\\'t mention methods or specifics. Therefore, the answer should state that the experiments show performance across grouped benchmarks for different model sizes, with the best scores selected. Since the individual benchmarks are in another section not provided, the info is limited here. So the answer should be based on that, and if unsure, say there\\'s not enough info. But the tables do have data, so maybe use that.\\n</think>\\n\\nThe context provided includes performance metrics for Llama 2 models (7B, 13B, 34B, 70B) across grouped benchmark categories, both in base configurations and after fine-tuning. For fine-tuning experiments, the results are presented as numerical values (e.g., 0.28, 0.25, 0.29, etc.) for different model sizes under the \"Fine-tuned\" section. However, the context does not explicitly describe the methodology, objectives, or specific benchmarks used in the fine-tuning experiments. \\n\\nThe text notes that results are grouped into categories (without naming them) and that \"the best score between our evaluation framework and any publicly reported results\" was selected. While the tables show performance improvements or variations for larger models (e.g., 70B outperforms 7B in some metrics), the lack of labeled benchmarks, detailed experimental setup, or analysis limits the ability to provide a comprehensive explanation. \\n\\n**Answer:**  \\nThe Llama 2 fine-tuning benchmark experiments, as summarized in the context, compare performance across grouped academic benchmarks for models of varying sizes (7B, 13B, 34B, 70B). The results indicate that larger models generally achieve higher scores in most categories, though specific benchmarks or methodologies are not detailed. The experiments report numerical performance metrics (e.g., 0.31, 0.29, 0.35) for each model size, but the exact nature of the benchmarks and fine-tuning procedures is not provided. For detailed analysis, the context directs readers to Section A.2.2, which is not included here.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f561811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5525c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = os.path.join(os.getcwd(), \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e5d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(logs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05539e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
